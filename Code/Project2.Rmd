---
title: "ST 558 Project 2"
author: "John Hinic & Fang Wu"
date: '2022-07-01'
params:
  filter_type: "bus"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(GGally)
library(caret)
#install.packages("devtools")
#library(devtools)
#install_github("AppliedDataSciencePartners/xgboostExplainer")
```

## Introduction 

The consumption of online news is expediting day by day due to the extensive adoption of smartphones and the rise of social networks. Online news can capture the eye of a signiﬁcant amount of Internet users within a brief period of your time. Prediction of online news popularity helps news organizations to gain better insights into the audience interests and to deliver more relevant and appealing content in a proactive manner. The company can allocate resources more wisely to prepare stories over their life cycle. Moreover, prediction of news popularity is also beneﬁcial for trend forecasting, understanding the collective human behavior, advertisers to propose more proﬁtable monetization techniques,and readers to ﬁlter the huge amount of information quickly and efﬁciently.

We are going to analyze and predict the number of shares within different data channel of interest using an online news data set from [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#) . This data set summarizes a heterogeneous set of features about articles published by Mashable in a period of two years.

* We are going to focuse on the following predictors:

  1. url: URL of the article (non-predictive)
  
  2. timedelta: Days between the article publication and the dataset acquisition (non-predictive)
  
  3. n_tokens_title: Number of words in the title

  4. n_tokens_content Number of words in the content

  5. n_unique_tokens: Rate of unique words in the content

  6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content

  7. num_hrefs: Number of links

  8. num_self_hrefs: Number of links to other articles published by Mashable

  9. num_imgs: Number of images

  10. num_videos: Number of videos

  11. average_token_length: Average length of the words in the content

  12. num_keywords: Number of keywords in the metadata

  13. self_reference_min_shares: Min. shares of referenced articles in Mashable

  14. self_reference_max_shares: Max. shares of referenced articles in Mashable

  15. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable

  16. global_subjectivity: Text subjectivity

  17. global_sentiment_polarity: Text sentiment polarity

  18. global_rate_positive_words: Rate of positive words in the content

  19. global_rate_negative_words: Rate of negative words in the content

  20. rate_positive_words: Rate of positive words among non-neutral tokens

  21. rate_negative_words: Rate of negative words among non-neutral tokens

  22. title_subjectivity: Title subjectivity

  23. title_sentiment_polarity: Title polarity

  24. abs_title_subjectivity: Absolute subjectivity level

  25. abs_title_sentiment_polarity: Absolute polarity level

  26. shares: Number of shares (target)

Stop Words usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.

## Prepare Data

We'll use the `readr` and `dplyr` packages from `tifyverse`. First, we are going to read in data as tibble using function `read_csv`. Next, in order to access different data channel of interest automatically, we are going to create a variable called `type`. Last we `filter` the data channel of interest using `params$` automatically.  

* Read in raw data and create new variable `type`

```{r}
# read in raw data
raw_data <- read_csv("C:/NCSU/Git/ST558_Project2/Data/OnlineNewsPopularity.csv") 

# create type column for different data channel
type_data <- raw_data %>% mutate(type=ifelse(data_channel_is_lifestyle==1, "lifestyle", ifelse(data_channel_is_entertainment==1, "entertainment", ifelse(data_channel_is_bus==1, "bus", ifelse(data_channel_is_socmed==1, "socmed", ifelse(data_channel_is_tech==1, "tech", ifelse(data_channel_is_world==1, "world", NA)))))))
```

* Subset data channel of interest automatically with `params`

```{r}
# select data for data channel of interest
library(dplyr)
target_data <- type_data %>% filter(type == params$filter_type) 
target_data
```

* Split data into train and test sets

```{r}
library(caret)
set.seed(100)
train_index <- createDataPartition(target_data$is_weekend, p=0.7, list=FALSE)
train <- target_data[train_index,]
test <- target_data[-train_index, ]
train
```

## Summarizations on train set

* descriptive statistics:

```{r}
summary(train %>% select(timedelta, n_tokens_title, n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, self_reference_avg_sharess, self_reference_min_shares, self_reference_max_shares, global_rate_negative_words, global_rate_positive_words, global_sentiment_polarity, global_subjectivity, rate_negative_words, rate_positive_words, title_subjectivity, title_sentiment_polarity, abs_title_sentiment_polarity, abs_title_subjectivity))
```

* Correlation between predictors

```{r}
library(corrplot)
Correlation <- cor(train %>% select(timedelta, n_tokens_title, n_tokens_content, n_unique_tokens, n_non_stop_unique_tokens, num_hrefs, num_self_hrefs, num_imgs, num_videos, average_token_length, num_keywords, self_reference_avg_sharess, self_reference_min_shares, self_reference_max_shares, global_rate_negative_words, global_rate_positive_words, global_sentiment_polarity, global_subjectivity, rate_negative_words, rate_positive_words, title_subjectivity, title_sentiment_polarity, abs_title_sentiment_polarity, abs_title_subjectivity))
corrplot(Correlation, type="upper", tl.pos="lt")
```

This plot help us to compare correlation between predictors. 

* summary across different day of the week

We are going to create a new variable named `weekday` and show share performance on different day of the week.

```{r}
# create predictor weekday 
train_day <- train %>% mutate(weekday=ifelse(weekday_is_monday==1, "Monday", ifelse(weekday_is_tuesday==1, "Tuesday", ifelse(weekday_is_wednesday==1, "Wednesday", ifelse(weekday_is_thursday==1, "Thursday", ifelse(weekday_is_friday==1, "Friday", ifelse(weekday_is_saturday==1, "Saturday", ifelse(weekday_is_sunday==1, "Sunday", NA))))))))

# shares on different day
train_day %>% group_by(weekday) %>% summarize(n=n(), min=min(shares), max=max(shares), avg=mean(shares), median=median(shares))
```

We can inspect the number of records on each day as well as the minimum, maximum, mean and median of shares on each day of the week from above table.

Now let's look at the count of shares on different day of the week.

```{r}
g <- ggplot(train_day %>% filter(shares<quantile(shares, p=0.75)), aes(x=shares))
g + geom_freqpoly(aes(color=weekday))
```

* Number of links to other articles published by Mashable

```{r}
g <- ggplot(train_day, aes(x=num_self_hrefs, y=shares) )
g + geom_point()
```

* Number of links to other articles published by Mashable

```{r}
g <- ggplot(train_day, aes(x=num_self_hrefs, y=shares) )
g + geom_point()
```

## Model

### Linear Regression

```{r}
library(caret)
mlFit <- train(shares~timedelta+n_tokens_title+num_hrefs+num_imgs+num_videos+rate_positive_words, data=train, method="lm", preProcess=c("center", "scale"), trControl=trainControl(method="cv", number=10))
mlFit
```



### Ensemble Tree

* Boosted Trees

```{r}
boostedFit <- train(shares~timedelta+n_tokens_title+num_hrefs+num_imgs+num_videos+rate_positive_words, data=train, method="bstTree", preProcess=c("center", "scale"), trControl=trainControl(method="cv", number=10), tuneGrid=data.frame(mstop=10, maxdepth=10, nu=1:10))
boostedFit
```

